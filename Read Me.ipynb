{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d093648-9e66-4a10-82d6-1ed1e41cf536",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "source": [
    "# Running Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a649ac9-39fc-47e1-9684-cc2fb76f3eb5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "source": [
    "## Data Location\n",
    "All project data is stored in the **data submissions container** under `/FileStore/tables/paris_project/deployment/`.\n",
    "\n",
    "## Quick Start - Main Experiment Only\n",
    "To run the main experiment with pre-processed data:\n",
    "1. Download only the **Diamond layer** data from `deployment/diamond/`\n",
    "2. Download the `utils.py` file to your Databricks workspace directory\n",
    "3. Run the notebook: **`5. Local Model Training and Evaluation`**\n",
    "\n",
    "## Full Replication - Complete Pipeline\n",
    "To replicate the entire data processing pipeline:\n",
    "1. Download the complete **deployment directory** (includes Bronze, Silver, Gold, and Diamond layers)\n",
    "2. Download the `utils.py` file to your Databricks workspace directory\n",
    "3. Run notebooks **sequentially in order 1-5**:\n",
    "   * Notebook 1: Data Collection and Integration\n",
    "   * Notebook 2: Data Cleaning and Feature Selection\n",
    "   * Notebook 3: Paris specific features\n",
    "   * Notebook 4: Training The Global Model\n",
    "   * Notebook 5: Main Experiment - Local model training and evaluation\n",
    "- This will generate the data files under /FileStore/tables/paris_project/ and save the original deployment files untouched. to run the experiment with your file, you'll need to switch the loaded address in the very begining of notebook 5.   \n",
    "\n",
    "## Important Notes\n",
    "* The `utils.py` file must be in the same directory as the experiment notebooks\n",
    "* For quick experimentation, use the Diamond layer data with Notebook 5\n",
    "* For full reproducibility, run the complete pipeline (Notebooks 1-5)\n",
    "\n",
    "-----\n",
    "Enjoy Paris! Bon voyage \uD83D\uDDFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eb3393f-0615-4941-85f3-eda90378dac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63139975-5d9e-4ebd-805c-b70fa0b4100f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_parquet_file_count_and_size(path_str):\n",
    "    \"\"\"\n",
    "    Analyzes the storage layout of a given path and\n",
    "    prints the number of parquet files and their average size in MB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path_str)\n",
    "        parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
    "        file_count = len(parquet_files)\n",
    "        \n",
    "        if file_count == 0:\n",
    "            print(f\"Path: {path_str} -> No parquet files found directly (might be nested directories).\")\n",
    "            return 0,0\n",
    "        \n",
    "        file_sizes = [f.size for f in parquet_files] \n",
    "        total_size_bytes = sum(file_sizes)\n",
    "        \n",
    "        return file_count, total_size_bytes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {path_str}: {e}\")\n",
    "        return 0,0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c16c6f-a558-4730-a96b-d4d174a58048",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "source": [
    "## Data Flow Architecture - Medallion Layers\n",
    "\n",
    "This project follows a **medallion architecture** with multiple data quality layers:\n",
    "\n",
    "### \uD83E\uDD49 **Bronze Layer**\n",
    "Raw data ingestion zone - stores data in its original format as received from source systems. Minimal transformations, preserving data lineage.\n",
    "\n",
    "### \uD83E\uDD48 **Silver Layer** \n",
    "Cleaned and validated data - applies data quality rules, deduplication, standardization, and basic transformations on target columns. Data is split into training and test sets.\n",
    "\n",
    "### \uD83E\uDD47 **Gold Layer**\n",
    "Aggregated and enriched data - contains business-level aggregations, metrics, and feature engineering for analytics and ML. Includes Paris-specific geographic features (metros, monuments).\n",
    "\n",
    "### \uD83D\uDC8E **Diamond Layer**\n",
    "Production-ready datasets with model predictions - contains local training and test sets from Gold layer with global model prediction columns. Ready for final experiment runs and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5001ed5-cc40-45f0-90b5-582236772ea1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify deployment directory structure"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nDEPLOYMENT DIRECTORY STRUCTURE\n============================================================\n\n\n\uD83D\uDCC1 BRONZE (4 items):\n  • global_org_df.parquet/ - 8 files, 13932.58 MB\n  • local_org_df.parquet/ - 1 files, 54.36 MB\n  • paris_metros_org.parquet/ - 1 files, 0.01 MB\n  • paris_monuments_org.parquet/ - 1 files, 0.00 MB\n\n\uD83D\uDCC1 SILVER (3 items):\n  • global_train_v2.parquet/ - 8 files, 9026.16 MB\n  • local_train_pool_v2.parquet/ - 1 files, 30.15 MB\n  • test_set_v2.parquet/ - 1 files, 7.44 MB\n\n\uD83D\uDCC1 GOLD (5 items):\n  • global_train_features_v4.parquet/ - 4 files, 53.04 MB\n  • local_train_pool_v4.parquet/ - 1 files, 1.77 MB\n  • local_train_pool_v5_with_paris_features.parquet/ - 1 files, 6.39 MB\n  • test_set_v4.parquet/ - 1 files, 0.51 MB\n  • test_set_v5_with_paris_features.parquet/ - 1 files, 1.77 MB\n\n\uD83D\uDCC1 DIAMOND (2 items):\n  • local_train_with_global_pred_v7.parquet/ - 1 files, 8.13 MB\n  • test_set_with_global_pred_v7.parquet/ - 1 files, 2.16 MB\n\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify the final deployment structure\n",
    "deployment_base = \"/FileStore/tables/paris_project/deployment\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEPLOYMENT DIRECTORY STRUCTURE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Check each layer\n",
    "layers_to_check = ['bronze', 'silver', 'gold', 'diamond']\n",
    "\n",
    "for layer in layers_to_check:\n",
    "    layer_path = f\"{deployment_base}/{layer}\"\n",
    "    try:\n",
    "        items = dbutils.fs.ls(layer_path)\n",
    "        print(f\"\\n\uD83D\uDCC1 {layer.upper()} ({len(items)} items):\")\n",
    "        for item in items:\n",
    "            n_files, size = get_parquet_file_count_and_size(item.path)\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            print(f\"  • {item.name} - {n_files} files, {size_mb:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ {layer.upper()}: Not found or empty\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read Me",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}